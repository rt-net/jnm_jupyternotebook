{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert SSD MobileNet from UFF (.uff) to TensorRT engine file (.engine)\n",
    "\n",
    "NVIDIA has a large selection of pre-trained SSD models in its [Hello AI World guide](https://github.com/dusty-nv/jetson-inference/releases/tag/model-mirror-190618).\n",
    "\n",
    "This tutorial shows how to convert them to TensorRT engine file.\n",
    "\n",
    "**NOTE**: NVIDIA has deprecated the Caffe Parser and UFF Parser in TensorRT 7.0. It is expected that converting to ONNX will be required in the future, but for now we will convert from UFF.  \n",
    "https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-7.html\n",
    "\n",
    "Reference: https://github.com/jkjung-avt/tensorrt_demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uff\n",
    "import tensorrt as trt\n",
    "import graphsurgeon as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "if trt.__version__[0] < '7':\n",
    "    try:\n",
    "        from jnmouse.ssd_tensorrt import load_flattenconcat_plugin\n",
    "        load_flattenconcat_plugin()\n",
    "    except:\n",
    "        import ctypes\n",
    "        import os\n",
    "        from pathlib import Path\n",
    "        import subprocess\n",
    "        import sys\n",
    "        command1 = [\"cp\", \"-r\", \"/usr/src/tensorrt/samples/python/uff_ssd/\", \".\"], Path().resolve() \n",
    "        command2 = [\"mkdir\", \"-p\", \"uff_ssd/build\"], Path().resolve()\n",
    "        command3 = [\"cmake\", \"..\"], \"{}/uff_ssd/build\".format(Path().resolve())\n",
    "        command4 = [\"make\"], \"{}/uff_ssd/build\".format(Path().resolve())\n",
    "        command5 = [\"chmod\", \"-x\", \"libflattenconcat.so\"], \"{}/uff_ssd/build\".format(Path().resolve())\n",
    "        command6 = [\"cp\", \"libflattenconcat.so\", \"../../libflattenconcat.so.{}\".format(trt.__version__[0])], \"{}/uff_ssd/build\".format(Path().resolve())\n",
    "        for commands in (command1, command2, command3, command4, command5, command6):\n",
    "            command, cwd = commands\n",
    "            print(command)\n",
    "            res = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=cwd)\n",
    "            print(res.stdout.decode(\"utf8\"))\n",
    "            if res.stderr: print(res.stderr.decode(\"utf8\"))\n",
    "\n",
    "        LIB_FILE = os.path.abspath(os.path.join(Path().resolve(), 'libflattenconcat.so.{}'.format(trt.__version__[0])))\n",
    "        ctypes.CDLL(LIB_FILE)\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# load plugins\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the pre-trained model and configure parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained ssd_mobilenet_v2_coco model. The URL is written in the following script.  \n",
    "https://github.com/dusty-nv/jetson-inference/blob/master/tools/download-models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD MobileNetV2 COCO\n",
    "!wget https://nvidia.box.com/shared/static/jcdewxep8vamzm71zajcovza938lygre.gz -O SSD-Mobilenet-v2.tar.gz\n",
    "!tar zxvf SSD-Mobilenet-v2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD MobileNetV2 COCO\n",
    "uff_model_path = \"SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff\"\n",
    "trt_engine_path = \"ssd_mobilenet_v2_coco_trt{}_t210.engine\".format(trt.__version__[0])\n",
    "\n",
    "print(trt_engine_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model into TensorRT .engine file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model into TensorRT engine\n",
    "print(\"build .engine file\")\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "\n",
    "    builder.max_workspace_size = 1 << 28\n",
    "    builder.max_batch_size = 1\n",
    "    builder.fp16_mode = True\n",
    "\n",
    "    parser.register_input('Input', (3, 300, 300))\n",
    "    parser.register_output('MarkOutput_0')\n",
    "\n",
    "    print(\"parsing model\")\n",
    "    parser.parse(uff_model_path, network)\n",
    "\n",
    "    print(\"building engine\")\n",
    "    with builder.build_cuda_engine(network) as engine:\n",
    "        print(\"saving engine\")\n",
    "        with open(trt_engine_path, 'wb') as f:\n",
    "            f.write(engine.serialize())\n",
    "\n",
    "print(\"built .engine file successfully\")\n",
    "print(\"compiled TensorRT engine successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
